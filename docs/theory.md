# 对比学习理论学习笔记

## 核心思想
对比学习是一种自监督学习方法，通过让模型区分相似（正样本）和不相似（负样本）的图像对来学习特征表示

## 拆解对比学习

### 对比学习要训练出什么？
- 训练出一个合格的**编码器**，使其能够服务于下游任务（分类任务等）
- 编码器的主要作用就是**特征提取**，一张图片经过编码器被输出为特征向量，实际上输出的是编码器对于这个图片的消化理解，所以能高效的应对下游任务
- 所以对比学习要训练出的就是这样一个用于理解图片的工具，而不是具体的任务
### 对比学习的优势是什么？
- **无监督学习**，可以充分利用大量的无标签图片进行训练
- **特征质量高**，忽略表层细节,关注本质逻辑
- 训练过程与下游任务无关，**通用性强**

### 对比学习训练思路
- 将输入的图像转化成一个特征空间，在个空间中，让相似的样本（正样本）距离更近，而让不相似的样本（负样本）举例更远，这也是“对比”的由来

### 对比学习流程
- 数据增强：通过对原始数据集进行一系列的随机操作，生成`训练集`。这是对比的前提，是在无监督条件下让模型认识不同的关键步骤
- 编码器提取特征：对`训练集`进行编码，输出`特征向量`
- 投影头变换：将`特征向量`输入投影头，得到`最终向量`（SimCLR模型开创，这里为了完整性默认为对比学习的通用点，MoCo v1就没有使用对比头）
- 计算对比损失：通过`损失函数`计算损失，更新编码器和投影头中的参数
### 投影头的作用是什么？
- 定义：一个小的MLP网络，接在编码器之后，用于将特征向量h映射到另一个空间z，对比损失在z空间计算
- 作用：进行特征向量的对比，实现对比学习核心功能
### 为什么不在编码器中进行对比？
- 编码器的最主要的作用是能够提取输出特征，通过将对比任务下放到投影头中，可以增强编码器对特征的敏感性，不让其为了专注于区分图像而牺牲掉有用的语用信息
- 过滤冗余信息（比如数据增强产生的噪声）
- 通过使用投影头专注于对比，能显著提升模型质量。在SimCLR论文中，引入投影头提升了7%的准确率

## SimCLR 模型
### SimCLR实现流程
- 数据增强
    - 对于一个`原始数据`样本，随即运用两个数据增强操作，生成两个相关的视图，构成一个样本对
    - 数据增强操作方法：
        1. 将图片随机裁剪后换回原来的尺寸（这是最重要的增强
        2. 随机颜色失真（与方法一组合有助于提高性能
        3. 随机高斯模糊
- 编码器提取特征
    - 通常使用Resnet-50
    - 对于一个batch的图像，这个参数是共享的
- 投影层变换
    - 一个多层感知机，包含一个隐藏层
- 计算对比损失
    - 损失函数:NT-Xent
    - 对于一个包含 \(N\) 个样本的 Batch，通过数据增强得到 \(2N\) 个视图。对于任意一个正样本对 \((i, j)\)，其损失定义为：

    $$
    \ell_{i,j} = -\log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)}
    $$


    - \(z_i, z_j\)：同一原图的两个不同增强视图的表示向量。
    - \(\text{sim}(u, v) = \frac{u^T v}{\|u\| \|v\|}\)：余弦相似度（即两个 **L2归一化** 向量的点积）。
    - \(\tau\)：温度参数，控制对困难负样本的惩罚强度。
    - \(\mathbb{1}_{[k \neq i]} \in \{0,1\}\)：指示函数，排除与自身比较。
    - 分母遍历 \(2N-1\) 个负样本。

        最终的 Batch 损失是所有正样本对损失的平均值：

    $$
    \mathcal{L} = \frac{1}{2N} \sum_{k=1}^{N} \left[ \ell_{2k-1, 2k} + \ell_{2k, 2k-1} \right]
    $$
    
### SimCLR实现注意事项
- 固定随机种子：保证实验的可复现性
## MoCo 模型
- 引入队列存储负样本，重用之前batch的编码特征。
- 使用动量编码器保持队列特征的一致性。
- 可以突破batch size限制。
